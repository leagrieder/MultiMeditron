<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Adding new modality - MultiMeditron 1.0.0 documentation</title><link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Dataset format" href="dataset_format.html" /><link rel="prev" title="Quickstart" href="quickstart.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1c6635e" />
    <link rel="stylesheet" type="text/css" href="../_static/shibuya.css?v=44020203" />
    <link media="print" rel="stylesheet" type="text/css" href="../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="Adding new modality"/>
    <meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../index.html">
      
      
      <strong>MultiMeditron</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="guide.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Adding new modality</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_format.html">Dataset format</a></li>
<li class="toctree-l2"><a class="reference internal" href="training.html">Training a MultiMeditron model</a></li>
<li class="toctree-l2"><a class="reference internal" href="known_issues.html">Known issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html">Configuration Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ref/modules.html">Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../ref/multimeditron.html">multimeditron package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.cli.html">multimeditron.cli package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.dataset.html">multimeditron.dataset package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ref/multimeditron.dataset.loader.html">multimeditron.dataset.loader package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.model.html">multimeditron.model package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ref/multimeditron.model.modalities.html">multimeditron.model.modalities package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.train.html">multimeditron.train package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.utils.html">multimeditron.utils package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.verl.html">multimeditron.verl package</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div class="localtoc"><h3>On this page</h3><ul>
<li><a class="reference internal" href="#implementation-example">Implementation example</a><ul>
<li><a class="reference internal" href="#modality-loader">Modality loader</a></li>
<li><a class="reference internal" href="#modality-configuration">Modality configuration</a></li>
<li><a class="reference internal" href="#modality-pre-processor">Modality (pre)processor</a></li>
<li><a class="reference internal" href="#modality-modeling">Modality modeling</a></li>
</ul>
</li>
</ul>
</div><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../index.html"><span itemprop="name">MultiMeditron</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="guide.html"><span itemprop="name">User Guides</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">Adding new modality</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="relative min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
  <article class="yue" role="main">
          <section id="adding-new-modality">
<span id="add-modality-label"></span><h1>Adding new modality<a class="headerlink" href="#adding-new-modality" title="Link to this heading">¶</a></h1>
<p>Structure of the repository:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span data-line="1">src
</span><span data-line="2">└── multimeditron
</span><span data-line="3">    ├── cli
</span><span data-line="4">    ├── config
</span><span data-line="5">    ├── dataset
</span><span data-line="6">    │   └── loader
</span><span data-line="7">    │       └── image
</span><span data-line="8">    ├── model
</span><span data-line="9">    │   ├── modalities
</span><span data-line="10">    │   └── projectors
</span><span data-line="11">    ├── train
</span><span data-line="12">    ├── utils
</span><span data-line="13">    └── verl
</span></pre></div>
</div>
<p>In order to add a new modality, we must first understand how the training pipeline process raw modalities:</p>
<ol class="arabic simple">
<li><p><strong>Modality loading</strong>: This step loads modality from the dataset and transforms it into a raw modality format (for instance image bytes).</p></li>
<li><p><strong>Modality preprocessing</strong>: This step transforms raw modality into <code class="code docutils literal notranslate"><span class="pre">torch.Tensor</span></code></p></li>
<li><p><strong>Modality embedding</strong>: This step is the <code class="code docutils literal notranslate"><span class="pre">forward</span></code> step of your modality embedder. It forwards the <code class="code docutils literal notranslate"><span class="pre">torch.Tensor</span></code> object of the preprocessing step to create a <code class="code docutils literal notranslate"><span class="pre">torch.Tensor</span></code>: the modality embedding.</p></li>
</ol>
<p>Note that:</p>
<ul class="simple">
<li><p>Step 1 is <strong>model agnostic</strong>, every model uses the same loading functions.</p></li>
<li><p>Step 2 and 3 are <strong>model dependent</strong></p></li>
</ul>
<p>This means that if you implement a model for an existing modality, you don’t need to implement the modality loading step.</p>
<section id="implementation-example">
<h2>Implementation example<a class="headerlink" href="#implementation-example" title="Link to this heading">¶</a></h2>
<p>To create a new modality embedder, you need to implement 3 classes:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../ref/multimeditron.dataset.loader.html#multimeditron.dataset.loader.BaseModalityLoader" title="multimeditron.dataset.loader.BaseModalityLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityLoader</span></code></a> (only if implementing a new modality type): The modality loader to load the modality from the dataset</p></li>
<li><p><a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModalityConfig" title="multimeditron.model.modalities.base.BaseModalityConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityConfig</span></code></a>: The configuration file for both the processor and the modality model</p></li>
<li><p><a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModalityProcessor" title="multimeditron.model.modalities.base.BaseModalityProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityProcessor</span></code></a>: The processor class to preprocess your modalities</p></li>
<li><p><a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModality" title="multimeditron.model.modalities.base.BaseModality"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModality</span></code></a>: The modality model that forward your modalities</p></li>
</ul>
<p>In this walkthrough, we will show how to load images and how to create a simple modality embedder.</p>
<section id="modality-loader">
<h3>Modality loader<a class="headerlink" href="#modality-loader" title="Link to this heading">¶</a></h3>
<p>Here is an example to load images from bytes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Union</span>
</span><span data-line="2"><span class="kn">from</span> <span class="nn">multimeditron.dataset.loader</span> <span class="kn">import</span> <span class="n">BaseModalityLoader</span><span class="p">,</span> <span class="n">AutoModalityLoader</span>
</span><span data-line="3"><span class="kn">from</span> <span class="nn">multimeditron.model.constants</span> <span class="kn">import</span> <span class="n">MODALITY_VALUE_KEY</span>
</span><span data-line="4"><span class="kn">import</span> <span class="nn">PIL</span>
</span><span data-line="5"><span class="kn">import</span> <span class="nn">io</span>
</span><span data-line="6">
</span><span data-line="7"><span class="nd">@AutoModalityLoader</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;raw-image&quot;</span><span class="p">)</span>
</span><span data-line="8"><span class="k">class</span> <span class="nc">RawImageLoader</span><span class="p">(</span><span class="n">BaseModalityLoader</span><span class="p">):</span>
</span><span data-line="9">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="10">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span data-line="11">
</span><span data-line="12">    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">Image</span><span class="p">:</span>
</span><span data-line="13">        <span class="n">image_bytes</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="n">MODALITY_VALUE_KEY</span><span class="p">][</span><span class="s2">&quot;bytes&quot;</span><span class="p">]</span>
</span><span data-line="14">        <span class="n">image</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">image_bytes</span><span class="p">))</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">)</span>
</span><span data-line="15">        <span class="k">return</span> <span class="n">image</span>
</span></pre></div>
</div>
<p>A modality loader should always inherit from <a class="reference internal" href="../ref/multimeditron.dataset.loader.html#multimeditron.dataset.loader.BaseModalityLoader" title="multimeditron.dataset.loader.BaseModalityLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityLoader</span></code></a> and be registered using the python annotation <code class="xref py py-meth docutils literal notranslate"><span class="pre">register()</span></code></p>
<p>The <code class="code docutils literal notranslate"><span class="pre">load</span></code> function has the following signature:</p>
<ul class="simple">
<li><p>Input: A dictionary that contains a key <code class="code docutils literal notranslate"><span class="pre">&quot;value&quot;</span></code>, i.e. <code class="code docutils literal notranslate"><span class="pre">{&quot;value&quot;</span> <span class="pre">:</span> <span class="pre">&lt;something&gt;}</span></code>. This is the case for every modality. The actual format of the value field depends on the dataset format. See <cite>TODO</cite></p></li>
<li><p>Output returns the raw modality (here a <code class="xref py py-class docutils literal notranslate"><span class="pre">PIL.Image.Image</span></code>).</p></li>
</ul>
</section>
<section id="modality-configuration">
<h3>Modality configuration<a class="headerlink" href="#modality-configuration" title="Link to this heading">¶</a></h3>
<p>The configuration, processor, model architecture follows the same philosophy as <a class="reference external" href="https://huggingface.co/docs/transformers/custom_models">Huggingface custom model</a>.</p>
<p>The configuration file configures both the processor and the modality:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="kn">from</span> <span class="nn">multimeditron.model.modalities.base</span> <span class="kn">import</span> <span class="n">BaseModality</span>
</span><span data-line="2">
</span><span data-line="3"><span class="k">class</span> <span class="nc">ImageConfig</span><span class="p">(</span><span class="n">BaseModalityConfig</span><span class="p">):</span>
</span><span data-line="4">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span><span data-line="5">        <span class="bp">self</span><span class="p">,</span>
</span><span data-line="6">        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4096</span><span class="p">,</span>
</span><span data-line="7">        <span class="n">max_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
</span><span data-line="8">        <span class="n">clip_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;openai/clip-vit-large-patch14&quot;</span><span class="p">,</span>
</span><span data-line="9">        <span class="n">projection_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mlp&quot;</span><span class="p">,</span>
</span><span data-line="10">        <span class="o">**</span><span class="n">kwargs</span>
</span><span data-line="11">    <span class="p">):</span>
</span><span data-line="12">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span data-line="13">            <span class="n">max_batch_size</span><span class="o">=</span><span class="n">max_batch_size</span><span class="p">,</span>
</span><span data-line="14">            <span class="n">modality_type</span><span class="o">=</span><span class="s2">&quot;image&quot;</span><span class="p">,</span>
</span><span data-line="15">            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
</span><span data-line="16">            <span class="n">kwargs</span><span class="o">=</span><span class="n">kwargs</span>
</span><span data-line="17">        <span class="p">)</span>
</span><span data-line="18">
</span><span data-line="19">        <span class="bp">self</span><span class="o">.</span><span class="n">clip_name</span> <span class="o">=</span> <span class="n">clip_name</span>
</span><span data-line="20">        <span class="bp">self</span><span class="o">.</span><span class="n">projection_type</span> <span class="o">=</span> <span class="n">projection_type</span>
</span></pre></div>
</div>
<p>Every configuration needs to inherit <a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModalityConfig" title="multimeditron.model.modalities.base.BaseModalityConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityConfig</span></code></a> and call the <code class="code docutils literal notranslate"><span class="pre">__init__</span></code> function from <code class="code docutils literal notranslate"><span class="pre">BaseModalityConfig</span></code> wth the arguments:</p>
<ul class="simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">max_batch_size</span></code>: the maximum amount of modalities that can be processed in a single batch by the <cite>forward</cite> function of the modality embedder</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">modality_type</span></code>: which modality type does this processor/modality pair handle. This field should match the <code class="code docutils literal notranslate"><span class="pre">&quot;type&quot;</span></code> field in the dataset. See <cite>TODO</cite></p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">hidden_size</span></code>: the projected shape of the modality embedder (i.e. the size of a LLM token embedding)</p></li>
</ul>
<p>This configuration can be arbitrarily expanded with any JSON-serializable attributes. See <a class="reference external" href="https://huggingface.co/docs/transformers/custom_models">Huggingface custom model</a></p>
</section>
<section id="modality-pre-processor">
<h3>Modality (pre)processor<a class="headerlink" href="#modality-pre-processor" title="Link to this heading">¶</a></h3>
<p>A modality processor preprocess modalities to transform the raw modality from the loading step (here a <code class="code docutils literal notranslate"><span class="pre">PIL.Image.Image</span></code>) into a <code class="code docutils literal notranslate"><span class="pre">torch.Tensor</span></code>. This processing phase is applied during the collator phase (unlike the forward pass of the <a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModality" title="multimeditron.model.modalities.base.BaseModality"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModality</span></code></a>)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="kn">from</span> <span class="nn">multimeditron.model.constants</span> <span class="kn">import</span> <span class="n">NUM_EMBEDDINGS_KEY</span><span class="p">,</span> <span class="n">MODALITY_VALUE_KEY</span>
</span><span data-line="2"><span class="kn">from</span> <span class="nn">multimeditron.model.modalities.base</span> <span class="kn">import</span> <span class="n">BaseModalityProcessor</span>
</span><span data-line="3"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoConfig</span>
</span><span data-line="4">
</span><span data-line="5"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
</span><span data-line="6">
</span><span data-line="7"><span class="k">class</span> <span class="nc">ImageProcessor</span><span class="p">(</span><span class="n">BaseModalityProcessor</span><span class="p">):</span>
</span><span data-line="8">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
</span><span data-line="9">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span data-line="10">        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">clip_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;clip_name must be specified in the config&quot;</span>
</span><span data-line="11">
</span><span data-line="12">        <span class="bp">self</span><span class="o">.</span><span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">clip_name</span><span class="p">)</span>
</span><span data-line="13">
</span><span data-line="14">        <span class="n">feature_extractor_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">clip_name</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="15">        <span class="bp">self</span><span class="o">.</span><span class="n">_num_patches_per_entry</span> <span class="o">=</span> <span class="p">(</span><span class="n">feature_extractor_config</span><span class="o">.</span><span class="n">vision_config</span><span class="o">.</span><span class="n">image_size</span> <span class="o">//</span> <span class="n">feature_extractor_config</span><span class="o">.</span><span class="n">vision_config</span><span class="o">.</span><span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span><span data-line="16">
</span><span data-line="17">    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
</span><span data-line="18">        <span class="n">processed_modality</span> <span class="o">=</span> <span class="n">modality</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span><span data-line="19">        <span class="n">image</span> <span class="o">=</span> <span class="n">modality</span><span class="p">[</span><span class="n">MODALITY_VALUE_KEY</span><span class="p">]</span>
</span><span data-line="20">
</span><span data-line="21">        <span class="n">processed_modality</span><span class="p">[</span><span class="n">MODALITY_VALUE_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_processor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span><span data-line="22">        <span class="n">processed_modality</span><span class="p">[</span><span class="n">NUM_EMBEDDINGS_KEY</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_patches_per_entry</span>
</span><span data-line="23">
</span><span data-line="24">        <span class="k">return</span> <span class="n">processed_modality</span>
</span></pre></div>
</div>
<p>Each processor must inherit <a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModalityProcessor" title="multimeditron.model.modalities.base.BaseModalityProcessor"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityProcessor</span></code></a> (which inherit from <code class="xref py py-class docutils literal notranslate"><span class="pre">ProcessorMixin</span></code>).</p>
<p>The modality processor must impement the <code class="xref py py-meth docutils literal notranslate"><span class="pre">process()</span></code> function. This function takes:</p>
<ul class="simple">
<li><p>A <code class="code docutils literal notranslate"><span class="pre">Dict</span></code>, this is exactly the output of the previous loading phase</p></li>
<li><p>This function returns the exact same <code class="code docutils literal notranslate"><span class="pre">Dict</span></code> with the preprocessed modality in the <code class="code docutils literal notranslate"><span class="pre">&quot;value&quot;</span></code> key</p></li>
</ul>
</section>
<section id="modality-modeling">
<h3>Modality modeling<a class="headerlink" href="#modality-modeling" title="Link to this heading">¶</a></h3>
<p>Lastly, we implement the modality model. This is the model that performs the forward pass during training. To optimize GPU throughput, you should only put operations that can be parallelized on GPU.</p>
<p>A modality class must inherit <a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModality" title="multimeditron.model.modalities.base.BaseModality"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModality</span></code></a> is typically created with 2 main modules:</p>
<ol class="arabic simple">
<li><p>A pretrained modality embedder (like a CLIP model): This module produces meaningful embeddings for given modalities</p></li>
<li><p>A tunable projection module (usually a simple MLP or a linear layer): This module map embeddings from the modality embedder to the LLM embedding space. The dimension of this embedding space is given by the <cite>hidden_size</cite> attribute of <a class="reference internal" href="../ref/multimeditron.model.modalities.html#multimeditron.model.modalities.BaseModalityConfig" title="multimeditron.model.modalities.base.BaseModalityConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModalityConfig</span></code></a></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="kn">from</span> <span class="nn">multimeditron.model.constants</span> <span class="kn">import</span> <span class="n">NUM_EMBEDDINGS_KEY</span><span class="p">,</span> <span class="n">MODALITY_VALUE_KEY</span>
</span><span data-line="2"><span class="kn">from</span> <span class="nn">multimeditron.model.modalities.base</span> <span class="kn">import</span> <span class="n">BaseModalityProcessor</span>
</span><span data-line="3"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoConfig</span>
</span><span data-line="4"><span class="kn">import</span> <span class="nn">torch</span>
</span><span data-line="5">
</span><span data-line="6"><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
</span><span data-line="7">
</span><span data-line="8"><span class="nd">@AutoModality</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;meditron_clip&quot;</span><span class="p">)</span>
</span><span data-line="9"><span class="k">class</span> <span class="nc">ImageModality</span><span class="p">(</span><span class="n">BaseModality</span><span class="p">):</span>
</span><span data-line="10">    <span class="n">config_class</span> <span class="o">=</span> <span class="n">ImageConfig</span>
</span><span data-line="11">    <span class="n">preprocessor_class</span> <span class="o">=</span> <span class="n">ImageProcessor</span>
</span><span data-line="12">
</span><span data-line="13">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">ImageConfig</span><span class="p">):</span>
</span><span data-line="14">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span><span data-line="15">
</span><span data-line="16">        <span class="bp">self</span><span class="o">.</span><span class="n">vision_tower_name</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">clip_name</span>
</span><span data-line="17">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">vision_tower_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;vision_tower_name must be specified in the config&quot;</span>
</span><span data-line="18">
</span><span data-line="19">        <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vision_tower_name</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span data-line="20">        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">vision_embed_dim</span>
</span><span data-line="21">        <span class="bp">self</span><span class="o">.</span><span class="n">_num_patches_per_entry</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">image_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">vision_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span><span data-line="22">
</span><span data-line="23">        <span class="bp">self</span><span class="o">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">MLPProjector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span><span data-line="24">
</span><span data-line="25">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
</span><span data-line="26">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span data-line="27">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span><span data-line="28">        <span class="n">image_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">vision_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>
</span><span data-line="29">
</span><span data-line="30">        <span class="n">projected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>
</span><span data-line="31">
</span><span data-line="32">        <span class="k">return</span> <span class="n">projected</span>
</span><span data-line="33">
</span><span data-line="34">    <span class="k">def</span> <span class="nf">freeze_modality_embedder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="35">    <span class="k">for</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span><span data-line="36">        <span class="n">parameters</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</span><span data-line="37">
</span><span data-line="38">    <span class="k">def</span> <span class="nf">unfreeze_modality_embedder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="39">        <span class="k">for</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span><span data-line="40">            <span class="n">parameters</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</span><span data-line="41">
</span><span data-line="42">    <span class="k">def</span> <span class="nf">unfreeze_projection</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span data-line="43">        <span class="k">for</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
</span><span data-line="44">            <span class="n">parameters</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</span></pre></div>
</div>
<p>A modality class must implement 3 functions:</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>: this is the definition of the forward pass (which include the forward of both the modality embedder and the projection module)</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">freeze_modality_embedder()</span></code>: this function freezes the parameters of the modality embedder only</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">unfreeze_modality_embedder()</span></code>: this function unfreezes the parameters of the modality embedder</p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">unfreeze_projection()</span></code>: this function unfreezes the parameters of the projection module</p></li>
</ul>
<p>Those “freezing” functions are used to train different part of the whole MultiMeditron architecture to ensure training stability.</p>
<p>TODO: Redirect to creating dataset + launching training</p>
</section>
</section>
</section>

        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"><div class="navigation-prev">
    <a href="quickstart.html">
      <i class="i-lucide chevron-left"></i>
      <div class="page-info">
        <span>Previous</span><div class="title">Quickstart</div></div>
    </a>
  </div><div class="navigation-next">
    <a href="dataset_format.html">
      <div class="page-info">
        <span>Next</span>
        <div class="title">Dataset format</div>
      </div>
      <i class="i-lucide chevron-right"></i>
    </a>
  </div></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, LiGHT laboratory</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../_static/documentation_options.js?v=8d563738"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/shibuya.js?v=cac61aee"></script></body>
</html>