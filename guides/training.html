<!DOCTYPE html>
<html lang="en" data-accent-color="violet" data-content_root="../">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Training a MultiMeditron model - MultiMeditron 1.0.0 documentation</title><link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Known issues" href="known_issues.html" /><link rel="prev" title="Dataset format" href="dataset_format.html" /><script>
    function setColorMode(t){let e=document.documentElement;e.setAttribute("data-color-mode",t);let a=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,s=t;"auto"===t&&(s=a?"dark":"light"),"light"===s?(e.classList.remove("dark"),e.classList.add("light")):(e.classList.remove("light"),e.classList.add("dark"))}
    setColorMode(localStorage._theme||"auto");
  </script><link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1c6635e" />
    <link rel="stylesheet" type="text/css" href="../_static/shibuya.css?v=44020203" />
    <link media="print" rel="stylesheet" type="text/css" href="../_static/print.css?v=20ff2c19" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --sy-f-text: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
  --sy-f-heading: "Inter", var(--sy-f-sys), var(--sy-f-cjk), sans-serif;
}
</style>
    <meta property="og:type" content="website"/><meta property="og:title" content="Training a MultiMeditron model"/>
    <meta name="twitter:card" content="summary"/>
  </head>
<body><div class="sy-head">
  <div class="sy-head-blur"></div>
  <div class="sy-head-inner sy-container mx-auto">
    <a class="sy-head-brand" href="../index.html">
      
      
      <strong>MultiMeditron</strong>
    </a>
    <div class="sy-head-nav" id="head-nav">
      <nav class="sy-head-links"></nav>
      <div class="sy-head-extra flex items-center print:hidden"><form class="searchbox flex items-center" action="../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <kbd>/</kbd>
</form><div class="sy-head-socials"></div></div>
    </div>
    <div class="sy-head-actions flex items-center shrink-0 print:hidden"><button class="js-theme theme-switch flex items-center"
data-aria-auto="Switch to light color mode"
data-aria-light="Switch to dark color mode"
data-aria-dark="Switch to auto color mode">
<i class="i-lucide theme-icon"></i>
</button><button class="md:hidden flex items-center js-menu" aria-label="Menu" type="button" aria-controls="head-nav" aria-expanded="false">
        <div class="hamburger">
          <span class="hamburger_1"></span>
          <span class="hamburger_2"></span>
          <span class="hamburger_3"></span>
        </div>
      </button>
    </div>
  </div>
</div>
<div class="sy-page sy-container flex mx-auto">
  <aside id="lside" class="sy-lside md:w-72 md:shrink-0 print:hidden">
    <div class="sy-lside-inner md:sticky">
      <div class="sy-scrollbar p-6">
        <div class="globaltoc" data-expand-depth="0"><ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="guide.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="add_modality.html">Adding new modality</a></li>
<li class="toctree-l2"><a class="reference internal" href="dataset_format.html">Dataset format</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Training a MultiMeditron model</a></li>
<li class="toctree-l2"><a class="reference internal" href="known_issues.html">Known issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html">Configuration Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ref/modules.html">Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../ref/multimeditron.html">multimeditron package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.cli.html">multimeditron.cli package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.dataset.html">multimeditron.dataset package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ref/multimeditron.dataset.loader.html">multimeditron.dataset.loader package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.model.html">multimeditron.model package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../ref/multimeditron.model.modalities.html">multimeditron.model.modalities package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.train.html">multimeditron.train package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.utils.html">multimeditron.utils package</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ref/multimeditron.verl.html">multimeditron.verl package</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </div>
  </aside>
  <div class="lside-overlay js-menu" role="button" aria-label="Close left sidebar" aria-controls="lside" aria-expanded="false"></div>
  <aside id="rside" class="sy-rside pb-3 w-64 shrink-0 order-last">
    <button class="rside-close js-menu xl:hidden" aria-label="Close Table of Contents" type="button" aria-controls="rside" aria-expanded="false">
      <i class="i-lucide close"></i>
    </button>
    <div class="sy-scrollbar sy-rside-inner px-6 xl:top-16 xl:sticky xl:pl-0 pt-6 pb-4"><div class="localtoc"><h3>On this page</h3><ul>
<li><a class="reference internal" href="#configuration-files">Configuration files</a></li>
<li><a class="reference internal" href="#launch-the-training">Launch the training</a><ul>
<li><a class="reference internal" href="#single-node-training">Single node training</a></li>
<li><a class="reference internal" href="#multi-node-training">Multi node training</a><ul>
<li><a class="reference internal" href="#slurm-cluster">SLURM cluster</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div><div id="ethical-ad-placement" data-ea-publisher="readthedocs"></div></div>
  </aside>
  <div class="rside-overlay js-menu" role="button" aria-label="Close Table of Contents" aria-controls="rside" aria-expanded="false"></div>
  <main class="sy-main w-full max-sm:max-w-full print:pt-6">
<div class="sy-breadcrumbs" role="navigation">
  <div class="sy-breadcrumbs-inner flex items-center">
    <div class="md:hidden mr-3">
      <button class="js-menu" aria-label="Menu" type="button" aria-controls="lside" aria-expanded="false">
        <i class="i-lucide menu"></i>
      </button>
    </div>
    <ol class="flex-1" itemscope itemtype="https://schema.org/BreadcrumbList"><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="../index.html"><span itemprop="name">MultiMeditron</span></a>
        <span>/</span>
        <meta itemprop="position" content="1" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <a itemprop="item" href="guide.html"><span itemprop="name">User Guides</span></a>
        <span>/</span>
        <meta itemprop="position" content="2" />
      </li><li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <strong itemprop="name">Training a MultiMeditron model</strong>
        <meta itemprop="position" content="3" />
      </li></ol>
    <div class="xl:hidden ml-1">
      <button class="js-menu" aria-label="Show table of contents" type="button" aria-controls="rside"
        aria-expanded="false">
        <i class="i-lucide outdent"></i>
      </button>
    </div>
  </div>
</div><div class="flex flex-col break-words justify-between">
      <div class="relative min-w-0 max-w-6xl px-6 pb-6 pt-8 xl:px-12">
  <article class="yue" role="main">
          <section id="training-a-multimeditron-model">
<h1>Training a MultiMeditron model<a class="headerlink" href="#training-a-multimeditron-model" title="Link to this heading">¶</a></h1>
<p>This tutorial provides a step-by-step guide on how to train a model using MultiMeditron. We will walk you through the process with clear examples.</p>
<section id="configuration-files">
<h2>Configuration files<a class="headerlink" href="#configuration-files" title="Link to this heading">¶</a></h2>
<p>Each training is configured through a YAML file. To get the full documentation of the different arguments supported by the configuration file, refer to <a class="reference internal" href="configuration.html#config-ref-label"><span class="std std-ref">the configuration reference</span></a></p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="nt">base_llm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Qwen/Qwen3-8B</span>
</span><span data-line="2"><span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
</span><span data-line="3"><span class="nt">attachment_token</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;|reserved_special_token_0|&gt;</span>
</span><span data-line="4"><span class="nt">tokenizer_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">qwen3</span>
</span><span data-line="5"><span class="nt">token_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4096</span>
</span><span data-line="6">
</span><span data-line="7"><span class="nt">loaders</span><span class="p">:</span>
</span><span data-line="8"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">loader_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">raw-image</span>
</span><span data-line="9"><span class="w">    </span><span class="nt">modality_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">image</span>
</span><span data-line="10">
</span><span data-line="11"><span class="nt">modalities</span><span class="p">:</span>
</span><span data-line="12"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">model_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">meditron_clip</span>
</span><span data-line="13"><span class="w">    </span><span class="nt">clip_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">openai/clip-vit-large-patch14</span>
</span><span data-line="14"><span class="w">    </span><span class="nt">hidden_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4096</span>
</span><span data-line="15">
</span><span data-line="16"><span class="nt">training_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ALIGNMENT</span>
</span><span data-line="17">
</span><span data-line="18"><span class="nt">datasets</span><span class="p">:</span>
</span><span data-line="19"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">packed_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/path/to/dataset</span>
</span><span data-line="20">
</span><span data-line="21"><span class="nt">training_args</span><span class="p">:</span>
</span><span data-line="22"><span class="w">  </span><span class="nt">output_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/path/to/checkpoint</span>
</span><span data-line="23"><span class="w">  </span><span class="nt">dataloader_num_workers</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16</span>
</span><span data-line="24"><span class="w">  </span><span class="nt">dataloader_prefetch_factor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span data-line="25"><span class="w">  </span><span class="nt">remove_unused_columns</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span data-line="26"><span class="w">  </span><span class="nt">ddp_find_unused_parameters</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span data-line="27"><span class="w">  </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0e-4</span>
</span><span data-line="28"><span class="w">  </span><span class="nt">bf16</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span data-line="29"><span class="w">  </span><span class="nt">per_device_train_batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span>
</span><span data-line="30"><span class="w">  </span><span class="nt">gradient_accumulation_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
</span><span data-line="31"><span class="w">  </span><span class="nt">num_train_epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span data-line="32"><span class="w">  </span><span class="nt">gradient_checkpointing</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span data-line="33"><span class="w">  </span><span class="nt">gradient_checkpointing_kwargs</span><span class="p">:</span>
</span><span data-line="34"><span class="w">    </span><span class="nt">use_reentrant</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</span><span data-line="35"><span class="w">  </span><span class="nt">save_strategy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">epochs</span>
</span><span data-line="36"><span class="w">  </span><span class="nt">max_grad_norm</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.0</span>
</span><span data-line="37"><span class="w">  </span><span class="nt">deepspeed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">deepspeed.json</span>
</span><span data-line="38"><span class="w">  </span><span class="nt">accelerator_config</span><span class="p">:</span>
</span><span data-line="39"><span class="w">    </span><span class="nt">dispatch_batches</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
</span><span data-line="40"><span class="w">  </span><span class="nt">lr_scheduler_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cosine_with_min_lr</span>
</span><span data-line="41"><span class="w">  </span><span class="nt">lr_scheduler_kwargs</span><span class="p">:</span>
</span><span data-line="42"><span class="w">    </span><span class="nt">min_lr</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3.0e-5</span>
</span><span data-line="43"><span class="w">  </span><span class="nt">logging_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</span><span data-line="44"><span class="w">  </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.01</span>
</span></pre></div>
</div>
<p>Make sure to replace <code class="code docutils literal notranslate"><span class="pre">/path/to/dataset</span></code> and <code class="code docutils literal notranslate"><span class="pre">/path/to/checkpoint</span></code> by your dataset and the actual output checkpoint path.
Store this file in a YAML file. In our case, we store it in <code class="code docutils literal notranslate"><span class="pre">config.yaml</span></code>.</p>
<p>Additionally, we are using Deepspeed for parallelism and we need to create a deepspeed config. Here is our config used on a NVidia GH200 setup:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="p">{</span>
</span><span data-line="2"><span class="w">     </span><span class="nt">&quot;bf16&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span data-line="3"><span class="w">         </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
</span><span data-line="4"><span class="w">     </span><span class="p">},</span>
</span><span data-line="5"><span class="w">     </span><span class="nt">&quot;zero_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span data-line="6"><span class="w">         </span><span class="nt">&quot;stage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
</span><span data-line="7"><span class="w">         </span><span class="nt">&quot;offload_optimizer&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span data-line="8"><span class="w">           </span><span class="nt">&quot;device&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
</span><span data-line="9"><span class="w">           </span><span class="nt">&quot;pin_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
</span><span data-line="10"><span class="w">         </span><span class="p">},</span>
</span><span data-line="11"><span class="w">         </span><span class="nt">&quot;overlap_comm&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
</span><span data-line="12"><span class="w">         </span><span class="nt">&quot;contiguous_gradients&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
</span><span data-line="13"><span class="w">         </span><span class="nt">&quot;reduce_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span data-line="14"><span class="w">         </span><span class="nt">&quot;stage3_prefetch_bucket_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span data-line="15"><span class="w">         </span><span class="nt">&quot;stage3_param_persistence_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span data-line="16"><span class="w">         </span><span class="nt">&quot;sub_group_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
</span><span data-line="17"><span class="w">         </span><span class="nt">&quot;stage3_max_live_parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
</span><span data-line="18"><span class="w">         </span><span class="nt">&quot;stage3_max_reuse_distance&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1e9</span><span class="p">,</span>
</span><span data-line="19"><span class="w">         </span><span class="nt">&quot;stage3_gather_16bit_weights_on_model_save&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
</span><span data-line="20"><span class="w">     </span><span class="p">},</span>
</span><span data-line="21"><span class="w">     </span><span class="nt">&quot;gradient_accumulation_steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span data-line="22"><span class="w">     </span><span class="nt">&quot;train_micro_batch_size_per_gpu&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
</span><span data-line="23"><span class="w">     </span><span class="nt">&quot;gradient_clipping&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>
</span><span data-line="24"><span class="w">     </span><span class="nt">&quot;wall_clock_breakdown&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
</span><span data-line="25"><span class="w">     </span><span class="nt">&quot;activation_checkpointing&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span data-line="26"><span class="w">         </span><span class="nt">&quot;partition_activations&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
</span><span data-line="27"><span class="w">         </span><span class="nt">&quot;contiguous_memory_optimization&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
</span><span data-line="28"><span class="w">         </span><span class="nt">&quot;cpu_checkpointing&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
</span><span data-line="29"><span class="w">     </span><span class="p">},</span>
</span><span data-line="30"><span class="w">     </span><span class="nt">&quot;flops_profiler&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span data-line="31"><span class="w">         </span><span class="nt">&quot;enabled&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
</span><span data-line="32"><span class="w">     </span><span class="p">},</span>
</span><span data-line="33"><span class="w">     </span><span class="nt">&quot;aio&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</span><span data-line="34"><span class="w">         </span><span class="nt">&quot;block_size&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1048576</span><span class="p">,</span>
</span><span data-line="35"><span class="w">         </span><span class="nt">&quot;queue_depth&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
</span><span data-line="36"><span class="w">         </span><span class="nt">&quot;single_submit&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
</span><span data-line="37"><span class="w">         </span><span class="nt">&quot;overlap_events&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
</span><span data-line="38"><span class="w">     </span><span class="p">}</span>
</span><span data-line="39"><span class="w"> </span><span class="p">}</span>
</span></pre></div>
</div>
<p>Store this file in <code class="code docutils literal notranslate"><span class="pre">deepspeed.json</span></code> file, make sure that the path to this file matches the <code class="code docutils literal notranslate"><span class="pre">training_args.deepspeed</span></code> argument from the YAML configuration.</p>
</section>
<section id="launch-the-training">
<h2>Launch the training<a class="headerlink" href="#launch-the-training" title="Link to this heading">¶</a></h2>
<p>Once the training configuration are done, we are ready to launch a training. We support both single node and multi node training.</p>
<section id="single-node-training">
<h3>Single node training<a class="headerlink" href="#single-node-training" title="Link to this heading">¶</a></h3>
<p>To launch a single training, run the following command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span data-line="1">torchrun<span class="w"> </span>--nproc-per-node<span class="w"> </span><span class="nv">$PROC_PER_NODE</span><span class="w"> </span>-m<span class="w"> </span>multimeditron<span class="w"> </span>train<span class="w"> </span>--config<span class="w"> </span>config.yaml
</span></pre></div>
</div>
<p>where <code class="code docutils literal notranslate"><span class="pre">$PROC_PER_NODE</span></code> is the number of GPUS available</p>
</section>
<section id="multi-node-training">
<h3>Multi node training<a class="headerlink" href="#multi-node-training" title="Link to this heading">¶</a></h3>
<p>We provide scripts to launch MultiMeditron training on multi node cluster. We provide scripts to launch trainings on:</p>
<ul class="simple">
<li><p>SLURM cluster</p></li>
<li><p>TODO: Provide script for Run:ai cluster</p></li>
</ul>
<section id="slurm-cluster">
<h4>SLURM cluster<a class="headerlink" href="#slurm-cluster" title="Link to this heading">¶</a></h4>
<p>To launch a training on a SLURM cluster, we can use the following <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span data-line="1"><span class="ch">#!/bin/bash</span>
</span><span data-line="2"><span class="w"> </span><span class="c1">#SBATCH --job-name multimeditron-training</span>
</span><span data-line="3"><span class="w"> </span><span class="c1">#SBATCH --output ~/reports/R-%x.%j.out</span>
</span><span data-line="4"><span class="w"> </span><span class="c1">#SBATCH --error ~/reports/R-%x.%j.err</span>
</span><span data-line="5"><span class="w"> </span><span class="c1">#SBATCH --nodes 4         # number of Nodes</span>
</span><span data-line="6"><span class="w"> </span><span class="c1">#SBATCH --ntasks-per-node 1     # number of MP tasks. IMPORTANT: torchrun represents just 1 Slurm task</span>
</span><span data-line="7"><span class="w"> </span><span class="c1">#SBATCH --gres gpu:4        # Number of GPUs</span>
</span><span data-line="8"><span class="w"> </span><span class="c1">#SBATCH --cpus-per-task 288     # number of CPUs per task.</span>
</span><span data-line="9"><span class="w"> </span><span class="c1">#SBATCH --time 11:59:59       # maximum execution time (DD-HH:MM:SS)</span>
</span><span data-line="10"><span class="w"> </span><span class="c1">#SBATCH --environment ~/.edf/multimodal.toml</span>
</span><span data-line="11"><span class="w"> </span><span class="c1">#SBATCH --export=ALL,SCRATCH=/iopsstor/scratch/cscs/$USER</span>
</span><span data-line="12"><span class="w"> </span><span class="c1">#SBATCH -A a127</span>
</span><span data-line="13">
</span><span data-line="14"><span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_LAUNCH_BLOCKING</span><span class="o">=</span><span class="m">1</span>
</span><span data-line="15"><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;START TIME: </span><span class="k">$(</span>date<span class="k">)</span><span class="s2">&quot;</span>
</span><span data-line="16"><span class="w"> </span><span class="c1"># auto-fail on any errors in this script</span>
</span><span data-line="17"><span class="w"> </span><span class="nb">set</span><span class="w"> </span>-eo<span class="w"> </span>pipefail
</span><span data-line="18"><span class="w"> </span><span class="c1"># logging script&#39;s variables/commands for future debug needs</span>
</span><span data-line="19"><span class="w"> </span><span class="nb">set</span><span class="w"> </span>-x
</span><span data-line="20"><span class="w"> </span><span class="c1">######################</span>
</span><span data-line="21"><span class="w"> </span><span class="c1">### Set enviroment ###</span>
</span><span data-line="22"><span class="w"> </span><span class="c1">######################</span>
</span><span data-line="23"><span class="w"> </span><span class="nv">GPUS_PER_NODE</span><span class="o">=</span><span class="m">4</span>
</span><span data-line="24"><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;NODES: </span><span class="nv">$SLURM_NNODES</span><span class="s2">&quot;</span>
</span><span data-line="25"><span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">HF_HOME</span><span class="o">=</span>/path/to/hf/home
</span><span data-line="26"><span class="w"> </span><span class="c1">######################</span>
</span><span data-line="27"><span class="w"> </span><span class="c1">#### Set network #####</span>
</span><span data-line="28"><span class="w"> </span><span class="c1">######################</span>
</span><span data-line="29"><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>scontrol<span class="w"> </span>show<span class="w"> </span>hostnames<span class="w"> </span><span class="nv">$SLURM_JOB_NODELIST</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="k">)</span>
</span><span data-line="30"><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="m">6200</span>
</span><span data-line="31"><span class="w"> </span><span class="c1">######################</span>
</span><span data-line="32"><span class="w"> </span><span class="c1"># note that we don&#39;t want to interpolate `\$SLURM_PROCID` till `srun` since otherwise all nodes will get</span>
</span><span data-line="33"><span class="w"> </span><span class="c1"># 0 and the launcher will hang</span>
</span><span data-line="34"><span class="w"> </span><span class="c1">#</span>
</span><span data-line="35"><span class="w"> </span><span class="c1"># same goes for `\$(hostname -s|tr -dc &#39;0-9&#39;)` - we want it to interpolate at `srun` time</span>
</span><span data-line="36">
</span><span data-line="37"><span class="w"> </span><span class="nv">LAUNCHER</span><span class="o">=</span><span class="s2">&quot;</span>
</span><span data-line="38"><span class="s2">   torchrun \</span>
</span><span data-line="39"><span class="s2">   --nproc_per_node </span><span class="nv">$GPUS_PER_NODE</span><span class="s2"> \</span>
</span><span data-line="40"><span class="s2">   --nnodes </span><span class="nv">$SLURM_NNODES</span><span class="s2"> \</span>
</span><span data-line="41"><span class="s2">   --node_rank \$SLURM_PROCID \</span>
</span><span data-line="42"><span class="s2">   --rdzv_endpoint </span><span class="nv">$MASTER_ADDR</span><span class="s2">:</span><span class="nv">$MASTER_PORT</span><span class="s2"> \</span>
</span><span data-line="43"><span class="s2">   --rdzv_backend c10d \</span>
</span><span data-line="44"><span class="s2">   --max_restarts 0 \</span>
</span><span data-line="45"><span class="s2">   --tee 3 \</span>
</span><span data-line="46"><span class="s2">   &quot;</span>
</span><span data-line="47">
</span><span data-line="48"><span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">CMD</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$LAUNCHER</span><span class="s2"> -m multimeditron train --config config.yaml&quot;</span>
</span><span data-line="49">
</span><span data-line="50"><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="nv">$CMD</span>
</span><span data-line="51">
</span><span data-line="52"><span class="w"> </span><span class="nv">SRUN_ARGS</span><span class="o">=</span><span class="s2">&quot; \</span>
</span><span data-line="53"><span class="s2">   --cpus-per-task </span><span class="nv">$SLURM_CPUS_PER_TASK</span><span class="s2"> \</span>
</span><span data-line="54"><span class="s2">   --jobid </span><span class="nv">$SLURM_JOB_ID</span><span class="s2"> \</span>
</span><span data-line="55"><span class="s2">   --wait 60 \</span>
</span><span data-line="56"><span class="s2">   -A a06 \</span>
</span><span data-line="57"><span class="s2">   --reservation=sai-a127</span>
</span><span data-line="58"><span class="s2">   &quot;</span>
</span><span data-line="59">
</span><span data-line="60"><span class="w"> </span><span class="c1"># bash -c is needed for the delayed interpolation of env vars to work</span>
</span><span data-line="61"><span class="w"> </span>srun<span class="w"> </span><span class="nv">$SRUN_ARGS</span><span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$CMD</span><span class="s2">&quot;</span>
</span><span data-line="62"><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;END TIME: </span><span class="k">$(</span>date<span class="k">)</span><span class="s2">&quot;</span>
</span></pre></div>
</div>
<p>Make sure to set your <code class="code docutils literal notranslate"><span class="pre">$HF_HOME</span></code> properly before launching the training. Models and datasets will be downloaded in this folder which can take many GB! Save this configuration file in a file called <code class="code docutils literal notranslate"><span class="pre">training.sh</span></code></p>
<p>Finally launch the training by running this command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span data-line="1">sbatch<span class="w"> </span>training.sh
</span></pre></div>
</div>
</section>
</section>
</section>
</section>

        </article><button class="back-to-top" type="button">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
  </svg>
  <span>Back to top</span>
</button><div class="navigation flex print:hidden"><div class="navigation-prev">
    <a href="dataset_format.html">
      <i class="i-lucide chevron-left"></i>
      <div class="page-info">
        <span>Previous</span><div class="title">Dataset format</div></div>
    </a>
  </div><div class="navigation-next">
    <a href="known_issues.html">
      <div class="page-info">
        <span>Next</span>
        <div class="title">Known issues</div>
      </div>
      <i class="i-lucide chevron-right"></i>
    </a>
  </div></div></div>
    </div>
  </main>
</div>
<footer class="sy-foot">
  <div class="sy-foot-inner sy-container mx-auto">
    <div class="sy-foot-reserved md:flex justify-between items-center">
      <div class="sy-foot-copyright"><p>2025, LiGHT laboratory</p>
  
  <p>
    Made with
    
    <a href="https://www.sphinx-doc.org/">Sphinx</a> and
    
    <a href="https://shibuya.lepture.com">Shibuya theme</a>.
  </p>
</div>
      <div class="sy-foot-socials"></div>
    </div>
  </div>
</footer>
      <script src="../_static/documentation_options.js?v=8d563738"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/shibuya.js?v=cac61aee"></script></body>
</html>