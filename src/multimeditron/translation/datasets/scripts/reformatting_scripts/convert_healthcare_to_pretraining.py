"""
Hindi and Punjabi Healthcare Dataset Converter

Converts Hindi and Punjabi healthcare CSV datasets into Meditron pretraining JSONL format.
Creates natural language medical case texts from structured patient records including
diagnosis, patient history, symptoms, treatment, and demographics.

Input: CSV files with medical patient records
Output: JSONL files with {"text": "...", "modalities": []}

Usage:
    python convert_healthcare_to_pretraining.py
"""

import os
import json
import pandas as pd


INPUT_FILES = {
    'hindi': 'src/multimeditron/translation/datasets/raw/healthcare_hindi_punjabi/hindi_dataset.csv',
    'punjabi': 'src/multimeditron/translation/datasets/raw/healthcare_hindi_punjabi/punjabi_dataset.csv'
}

OUTPUT_DIR = 'src/multimeditron/translation/datasets/formatted_datasets/healthcare_datasets/healthcare_hindi_punjabi'
os.makedirs(OUTPUT_DIR, exist_ok=True)


def detect_language(row):
    """Detect language from gender field text content."""
    gender = str(row.get('gender', '')).strip()
    if '‡§™‡•Å‡§∞‡•Å‡§∑' in gender or '‡§Æ‡§π‡§ø‡§≤‡§æ' in gender:
        return 'Hindi'
    elif '‡®Æ‡®∞‡®¶' in gender or '‡®î‡®∞‡®§' in gender:
        return 'Punjabi'
    return 'Unknown'


def create_pretraining_text(row):
    """
    Create natural medical case text from patient record.
    Combines diagnosis, patient history, symptoms, treatment, etc.
    """
    language = detect_language(row)
    text_parts = []
    
    diagnosis = str(row.get('Diagnosis', '')).strip() if pd.notna(row.get('Diagnosis')) else ''
    patient_history = str(row.get('Patient History', '')).strip() if pd.notna(row.get('Patient History')) else ''
    symptoms = str(row.get('symptoms', '')).strip() if pd.notna(row.get('symptoms')) else ''
    treatment = str(row.get('treatment', '')).strip() if pd.notna(row.get('treatment')) else ''
    remarks = str(row.get('Remarks', '')).strip() if pd.notna(row.get('Remarks')) else ''
    timespan = str(row.get('timespan', '')).strip() if pd.notna(row.get('timespan')) else ''
    diagnosis_category = str(row.get('Diagnosis Category', '')).strip() if pd.notna(row.get('Diagnosis Category')) else ''
    age = str(row.get('age', '')).strip() if pd.notna(row.get('age')) else ''
    gender = str(row.get('gender', '')).strip() if pd.notna(row.get('gender')) else ''
    
    if diagnosis:
        if language == 'Hindi':
            text_parts.append(f"‡§®‡§ø‡§¶‡§æ‡§®: {diagnosis}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®®‡®ø‡®¶‡®æ‡®®: {diagnosis}")
        else:
            text_parts.append(f"Diagnosis: {diagnosis}")
    
    if age and gender:
        if language == 'Hindi':
            text_parts.append(f"‡§∞‡•ã‡§ó‡•Ä ‡§µ‡§ø‡§µ‡§∞‡§£: {age} ‡§µ‡§∞‡•ç‡§∑, {gender}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®Æ‡®∞‡©Ä‡®ú‡®º ‡®µ‡©á‡®∞‡®µ‡®æ: {age} ‡®∏‡®æ‡®≤, {gender}")
        else:
            text_parts.append(f"Patient: {age} years, {gender}")
    
    if patient_history:
        if language == 'Hindi':
            text_parts.append(f"‡§∞‡•ã‡§ó‡•Ä ‡§ï‡§æ ‡§á‡§§‡§ø‡§π‡§æ‡§∏: {patient_history}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®Æ‡®∞‡©Ä‡®ú‡®º ‡®¶‡®æ ‡®á‡®§‡®ø‡®π‡®æ‡®∏: {patient_history}")
        else:
            text_parts.append(f"Patient History: {patient_history}")
    
    if symptoms:
        if language == 'Hindi':
            text_parts.append(f"‡§≤‡§ï‡•ç‡§∑‡§£: {symptoms}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®≤‡©±‡®õ‡®£: {symptoms}")
        else:
            text_parts.append(f"Symptoms: {symptoms}")
    
    if treatment:
        if language == 'Hindi':
            text_parts.append(f"‡§â‡§™‡§ö‡§æ‡§∞: {treatment}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®á‡®≤‡®æ‡®ú: {treatment}")
        else:
            text_parts.append(f"Treatment: {treatment}")
    
    if timespan:
        if language == 'Hindi':
            text_parts.append(f"‡§â‡§™‡§ö‡§æ‡§∞ ‡§Ö‡§µ‡§ß‡§ø: {timespan}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®á‡®≤‡®æ‡®ú ‡®¶‡©Ä ‡®Æ‡®ø‡®Ü‡®¶: {timespan}")
        else:
            text_parts.append(f"Treatment Timeline: {timespan}")
    
    if remarks:
        if language == 'Hindi':
            text_parts.append(f"‡§ü‡§ø‡§™‡•ç‡§™‡§£‡•Ä: {remarks}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®ü‡®ø‡©±‡®™‡®£‡©Ä: {remarks}")
        else:
            text_parts.append(f"Remarks: {remarks}")
    
    if diagnosis_category:
        if language == 'Hindi':
            text_parts.append(f"‡§∂‡•ç‡§∞‡•á‡§£‡•Ä: {diagnosis_category}")
        elif language == 'Punjabi':
            text_parts.append(f"‡®∏‡®º‡©ç‡®∞‡©á‡®£‡©Ä: {diagnosis_category}")
        else:
            text_parts.append(f"Category: {diagnosis_category}")
    
    full_text = " ".join(text_parts)
    return full_text.strip()


def convert_csv_to_jsonl(csv_path, output_path, language):
    """Convert CSV to pretraining JSONL format."""
    print(f"\nüîÑ Processing {language} dataset: {csv_path}")
    
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding='latin-1')
    
    print(f"   Loaded {len(df)} rows")
    print(f"   Columns: {list(df.columns)}")
    
    valid_count = 0
    skipped_count = 0
    total_chars = 0
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for idx, row in df.iterrows():
            text = create_pretraining_text(row)
            
            if not text or len(text.strip()) < 50:
                skipped_count += 1
                continue
            
            entry = {
                "text": text,
                "modalities": []
            }
            
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            valid_count += 1
            total_chars += len(text)
    
    avg_length = total_chars / valid_count if valid_count > 0 else 0
    print(f"   ‚úÖ Wrote {valid_count} examples")
    print(f"   üìè Average text length: {avg_length:.0f} characters")
    print(f"   ‚ö†Ô∏è  Skipped {skipped_count} examples (too short)")
    print(f"   üìÅ Output: {output_path}")
    
    return valid_count, skipped_count


def main():
    print("="*80)
    print("üè• Healthcare Dataset ‚Üí Pretraining JSONL Converter")
    print("="*80)
    
    total_valid = 0
    total_skipped = 0
    
    for language, input_file in INPUT_FILES.items():
        output_file = os.path.join(OUTPUT_DIR, f'healthcare_{language}_pretraining.jsonl')
        
        if not os.path.exists(input_file):
            print(f"\n‚ùå File not found: {input_file}")
            print(f"   Please ensure the file is in the correct location.")
            continue
        
        valid, skipped = convert_csv_to_jsonl(input_file, output_file, language)
        total_valid += valid
        total_skipped += skipped
    
    print("\n" + "="*80)
    print("üìä CONVERSION SUMMARY")
    print("="*80)
    print(f"‚úÖ Total examples created: {total_valid}")
    print(f"‚ö†Ô∏è  Total examples skipped: {total_skipped}")
    print(f"üìÅ Output directory: {OUTPUT_DIR}/")
    print("\n‚ú® Conversion complete!")


if __name__ == "__main__":
    main()