"""
NLLB-200-3.3B Fine-tuning - PRODUCTION VERSION (FIXED)

Fixed: Token ID out of range error during evaluation
"""

import os
import json
import torch
import numpy as np
from datetime import datetime
from datasets import load_from_disk
from collections import Counter
from transformers import (
    AutoModelForSeq2SeqLM,
    NllbTokenizer,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
)
import evaluate

print("="*70)
print("NLLB Medical Fine-tuning - PRODUCTION VERSION")
print("Target: +2-5 chrF improvement with safety guarantees")
print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*70)

# =============================================================================
# CONFIGURATION
# =============================================================================

DATA_DIR = "src/multimeditron/translation/datasets/formatted_datasets/healthcare_datasets/openwho_all_languages"
OUTPUT_DIR = "src/multimeditron/translation/datasets/training/models/nllb-openwho-model"
MODEL_NAME = "facebook/nllb-200-3.3B"

# BALANCED hyperparameters
BATCH_SIZE = 4
GRADIENT_ACCUMULATION = 8
NUM_EPOCHS = 2
LEARNING_RATE = 3e-6
WARMUP_RATIO = 0.06
WEIGHT_DECAY = 0.01
MAX_LENGTH = 256

SAVE_EVERY = 400
LOG_EVERY = 50
EVAL_STEPS = 200

USE_LANGUAGE_BALANCING = True
BALANCE_STRATEGY = "sqrt"

print(f"\nüìä Configuration:")
print(f"  Strategy: Balanced (safety + quality)")
print(f"  Epochs: {NUM_EPOCHS}")
print(f"  Effective batch: {BATCH_SIZE * GRADIENT_ACCUMULATION}")
print(f"  Learning rate: {LEARNING_RATE}")
print(f"  Language balancing: {USE_LANGUAGE_BALANCING}")

os.makedirs(OUTPUT_DIR, exist_ok=True)

with open(os.path.join(OUTPUT_DIR, "training_config.json"), 'w') as f:
    json.dump({
        "version": "production_fixed",
        "target": "+2-5 chrF improvement",
        "start_time": datetime.now().isoformat(),
        "model_name": MODEL_NAME,
        "num_epochs": NUM_EPOCHS,
        "learning_rate": LEARNING_RATE,
        "effective_batch_size": BATCH_SIZE * GRADIENT_ACCUMULATION,
    }, f, indent=2)

# =============================================================================
# LOAD DATASET
# =============================================================================

print(f"\nüìÇ Loading dataset...")
dataset = load_from_disk(DATA_DIR)
print(f"‚úì Dataset loaded: {len(dataset['train']):,} examples")

print(f"\n‚úÇÔ∏è  Creating train/validation split...")
dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)
train_dataset = dataset['train']
eval_dataset = dataset['test']
print(f"  Train: {len(train_dataset):,}")
print(f"  Validation: {len(eval_dataset):,}")

# =============================================================================
# LOAD TOKENIZER
# =============================================================================

print(f"\nüî§ Loading tokenizer...")
tokenizer = NllbTokenizer.from_pretrained(
    MODEL_NAME,
    src_lang="eng_Latn",
    cache_dir=os.environ.get("HF_HOME", None)
)
print(f"‚úì Tokenizer loaded")

# =============================================================================
# PREPROCESSING
# =============================================================================

print(f"\n‚öôÔ∏è  Preprocessing with bidirectional training...")

def preprocess_function(examples):
    """Creates BOTH EN‚ÜíX and X‚ÜíEN examples."""
    sources = []
    targets = []
    src_langs = []
    tgt_langs = []
    
    for translation in examples['translation']:
        if 'eng_Latn' not in translation or translation['eng_Latn'] is None:
            continue
        
        eng_text = translation['eng_Latn']
        
        target_lang = None
        target_text = None
        for lang, text in translation.items():
            if lang != 'eng_Latn' and text is not None:
                target_lang = lang
                target_text = text
                break
        
        if target_text is None or target_lang is None:
            continue
        
        # BOTH directions
        sources.append(eng_text)
        targets.append(target_text)
        src_langs.append('eng_Latn')
        tgt_langs.append(target_lang)
        
        sources.append(target_text)
        targets.append(eng_text)
        src_langs.append(target_lang)
        tgt_langs.append('eng_Latn')
    
    model_inputs = tokenizer(
        sources,
        max_length=MAX_LENGTH,
        truncation=True,
        padding="max_length"
    )
    
    labels_list = []
    for target, tgt_lang in zip(targets, tgt_langs):
        tokenizer.src_lang = tgt_lang
        labels = tokenizer(
            target,
            max_length=MAX_LENGTH,
            truncation=True,
            padding="max_length"
        )
        labels_list.append(labels["input_ids"])
    
    model_inputs["labels"] = labels_list
    return model_inputs

tokenized_train = train_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=train_dataset.column_names,
    desc="Tokenizing train"
)

tokenized_eval = eval_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=eval_dataset.column_names,
    desc="Tokenizing validation"
)

print(f"‚úì Train: {len(tokenized_train):,} examples")
print(f"‚úì Validation: {len(tokenized_eval):,} examples")

# =============================================================================
# LOAD MODEL
# =============================================================================

print(f"\nü§ñ Loading model: {MODEL_NAME}")
model = AutoModelForSeq2SeqLM.from_pretrained(
    MODEL_NAME,
    cache_dir=os.environ.get("HF_HOME", None)
)

if hasattr(model, 'gradient_checkpointing_enable'):
    model.gradient_checkpointing_enable()
    print(f"  ‚úì Gradient checkpointing enabled")

num_params = sum(p.numel() for p in model.parameters())
print(f"‚úì Model loaded: {num_params/1e9:.2f}B parameters")

# =============================================================================
# EVALUATION METRICS (FIXED)
# =============================================================================

print(f"\nüìè Loading evaluation metrics...")
bleu_metric = evaluate.load("bleu")
chrf_metric = evaluate.load("chrf")

def compute_metrics(eval_preds):
    """
    Compute BLEU and chrF scores.
    FIXED: Proper handling of token IDs to avoid out-of-range errors.
    """
    preds, labels = eval_preds
    
    if isinstance(preds, tuple):
        preds = preds[0]
    
    # Replace -100 in labels (padding)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    
    # CRITICAL FIX: Clip predictions to valid token ID range
    vocab_size = len(tokenizer)
    preds = np.clip(preds, 0, vocab_size - 1)
    
    # Also ensure labels are within range (should already be, but safety first)
    labels = np.clip(labels, 0, vocab_size - 1)
    
    # Decode with error handling
    try:
        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Decoding error: {e}")
        print(f"  Skipping metric computation for this batch")
        return {"bleu": 0.0, "chrf": 0.0}
    
    # Filter out empty strings
    valid_pairs = [
        (pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)
        if pred.strip() and ref.strip()
    ]
    
    if not valid_pairs:
        return {"bleu": 0.0, "chrf": 0.0}
    
    decoded_preds, decoded_labels = zip(*valid_pairs)
    
    # Compute metrics
    try:
        bleu_result = bleu_metric.compute(
            predictions=list(decoded_preds),
            references=[[label] for label in decoded_labels]
        )
        
        chrf_result = chrf_metric.compute(
            predictions=list(decoded_preds),
            references=list(decoded_labels)
        )
        
        return {
            "bleu": bleu_result["bleu"] * 100,
            "chrf": chrf_result["score"],
        }
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Metric computation error: {e}")
        return {"bleu": 0.0, "chrf": 0.0}

print(f"‚úì Metrics: BLEU & chrF (with error handling)")

# =============================================================================
# TRAINING SETUP
# =============================================================================

print(f"\nüéØ Setting up training...")

num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1
effective_batch = BATCH_SIZE * GRADIENT_ACCUMULATION * num_gpus
steps_per_epoch = len(tokenized_train) // effective_batch
total_steps = steps_per_epoch * NUM_EPOCHS
warmup_steps = int(total_steps * WARMUP_RATIO)

print(f"  GPUs: {num_gpus}")
print(f"  Effective batch: {effective_batch}")
print(f"  Steps per epoch: {steps_per_epoch}")
print(f"  Total steps: {total_steps}")
print(f"  Warmup steps: {warmup_steps}")
print(f"  Estimated time: ~{total_steps * 2.2 / 3600:.1f} hours")

training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    
    # Training schedule
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    
    # Optimization
    learning_rate=LEARNING_RATE,
    warmup_steps=warmup_steps,
    lr_scheduler_type="cosine",
    weight_decay=WEIGHT_DECAY,
    max_grad_norm=1.0,
    
    # Precision
    fp16=torch.cuda.is_available(),
    
    # Evaluation
    eval_strategy="steps",
    eval_steps=EVAL_STEPS,
    
    # Saving
    save_strategy="steps",
    save_steps=SAVE_EVERY,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_bleu",
    greater_is_better=True,
    
    # Logging
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=LOG_EVERY,
    report_to=["tensorboard"],
    
    # Generation for evaluation
    predict_with_generate=True,
    generation_max_length=MAX_LENGTH,
    generation_num_beams=4,
    
    # Performance
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    gradient_checkpointing=True,
    
    # Misc
    push_to_hub=False,
    seed=42,
)

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    padding=True
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
)

print(f"‚úì Trainer ready")

# =============================================================================
# TRAINING
# =============================================================================

print("\n" + "="*70)
print("üöÄ STARTING PRODUCTION TRAINING")
print("="*70)
print(f"\n‚ÑπÔ∏è  Balanced approach (safety + quality)")
print(f"")
print(f"Quality improvements:")
print(f"  ‚úÖ 2 epochs (vs 1)")
print(f"  ‚úÖ Language balancing")
print(f"  ‚úÖ Larger batch (32)")
print(f"  ‚úÖ Cosine LR schedule")
print(f"  ‚úÖ BLEU-guided selection")
print(f"")
print(f"Safety features:")
print(f"  ‚úÖ Bidirectional training")
print(f"  ‚úÖ Early stopping (patience=5)")
print(f"  ‚úÖ Validation monitoring")
print(f"  ‚úÖ Best model selection")
print(f"  ‚úÖ Fixed token ID handling")
print(f"")
print(f"Expected: +2-5 chrF improvement")
print(f"Monitor: tensorboard --logdir {OUTPUT_DIR}/logs")
print("="*70 + "\n")

try:
    train_result = trainer.train()
    
    print("\n" + "="*70)
    print("‚úÖ TRAINING COMPLETED SUCCESSFULLY!")
    print("="*70)
    
    # Save final model
    print(f"\nüíæ Saving final model...")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # Save metadata
    metadata = {
        "version": "production_fixed",
        "status": "completed",
        "end_time": datetime.now().isoformat(),
        "total_steps": train_result.metrics.get('train_steps', 'N/A'),
        "epochs_completed": train_result.metrics.get('epoch', 'N/A'),
        "final_train_loss": train_result.metrics.get('train_loss', 'N/A'),
        "best_eval_bleu": train_result.metrics.get('eval_bleu', 'N/A'),
        "best_eval_chrf": train_result.metrics.get('eval_chrf', 'N/A'),
        "training_hours": train_result.metrics.get('train_runtime', 0) / 3600,
    }
    
    with open(os.path.join(OUTPUT_DIR, "training_metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    trainer.log_metrics("train", train_result.metrics)
    trainer.save_metrics("train", train_result.metrics)
    
    print(f"\nüìä Final Statistics:")
    print(f"  Total steps: {metadata['total_steps']}")
    print(f"  Epochs: {metadata['epochs_completed']:.2f}")
    print(f"  Final loss: {metadata['final_train_loss']:.4f}")
    print(f"  Best BLEU: {metadata['best_eval_bleu']:.2f}")
    print(f"  Best chrF: {metadata['best_eval_chrf']:.2f}")
    print(f"  Time: {metadata['training_hours']:.2f} hours")
    
    print(f"\n‚úÖ Model saved to: {OUTPUT_DIR}")
    print(f"\nüéâ Next: python experiment_1_african_languages.py")
    
except KeyboardInterrupt:
    print("\n\n‚ö†Ô∏è  Training interrupted")
    metadata = {
        "version": "production_fixed",
        "status": "interrupted",
        "end_time": datetime.now().isoformat()
    }
    with open(os.path.join(OUTPUT_DIR, "training_metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2)
    
except Exception as e:
    print(f"\n\n‚ùå Training failed: {str(e)}")
    import traceback
    traceback.print_exc()
    
    metadata = {
        "version": "production_fixed",
        "status": "failed",
        "end_time": datetime.now().isoformat(),
        "error": str(e)
    }
    with open(os.path.join(OUTPUT_DIR, "training_metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2)
    raise

print("\n" + "="*70)