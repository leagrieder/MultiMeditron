defaults:
  - _self_

source:
  type: jsonl # Supported types: 'hf', 'jsonl', 'parquet', 'csv'

  # Kwargs to pass to the dataset loading function
  # For 'hf', you are likely to want to use 'path' (with 'split', 'revision')
  # For 'jsonl', 'parquet', 'csv', use 'path'
  kwargs:
    path: ./mock_dataset/mock_dataset.jsonl

tokenizer:
  # Wether to enable tokenization or not
  enable: false

  # If enable, the tokenizer model to use (from HuggingFace)
  model: null

  # Whether to use the fast tokenizer implementation
  use_fast: true

  # Special token to use for attachments
  attachment_token: <|reserved_special_token_0|>

  # DType of the token (e.g., float32, bfloat16)
  dtype: bfloat16

output: ./mock_dataset/mock_dataset.parquet
num_processes: 8

# A list of all of the processing steps to apply to the dataset
processes:
  # This is an example of a custom python function to apply to each example (WARNING: using 'eval' is potentially unsafe)
  - type: python
    kwargs:
      func: |
        {
          "prompt": data["prompt"],
          "response": data["response"],
        }

  # This is an example of a custom python filter to apply to each example (WARNING: using 'eval' is potentially unsafe)
  - type: python-filter
    kwargs:
      func: |
        idx <= 5

  # This is an example of using the built-in shuffle processor
  - type: shuffle
    kwargs:
      seed: 42